# llm-serving-engine
A local-first LLM inference and serving engine with batching, scheduling, and streaming support.
